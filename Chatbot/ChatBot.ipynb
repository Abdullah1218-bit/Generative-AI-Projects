{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58049879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['Google AI Key'] = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542f8c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"LangChain_PDF_QA_Chatbot_Guide.pdf\")\n",
    "\n",
    "document=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5136396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2026-01-02T14:23:42+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-02T14:23:42+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'LangChain_PDF_QA_Chatbot_Guide.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Building a PDF-Based Q&A; Chatbot using LangChain\\n1. Introduction\\nThis document is designed as an ideal knowledge source for building a Question & Answer chatbot\\nusing LangChain. It is structured, text-based, and optimized for document loading, chunking, and\\nvector retrieval.\\n2. What is a Q&A; Chatbot?\\nA Q&A; chatbot is an AI system that answers user questions by retrieving relevant information from\\na knowledge base and generating accurate responses using a Large Language Model (LLM).\\n3. LangChain Overview\\nLangChain is a framework that helps developers build applications powered by language models. It\\nsupports document loading, text splitting, embeddings, vector databases, and retrieval-augmented\\ngeneration (RAG).\\n4. PDF Processing Pipeline\\n\\x7f\\nLoad PDF documents using a document loader\\n\\x7f\\nSplit text into manageable chunks\\n\\x7f\\nConvert text chunks into embeddings\\n\\x7f\\nStore embeddings in a vector database\\n\\x7f\\nRetrieve relevant chunks based on user queries\\n\\x7f\\nGenerate answers using an LLM\\n5. Best Practices for PDFs\\n\\x7f\\nUse text-based PDFs, not scanned images\\n\\x7f\\nMaintain clear headings and structure\\n\\x7f\\nKeep content domain-specific\\n\\x7f\\nAvoid unnecessary headers and footers\\n\\x7f\\nSplit large documents into sections\\n6. Recommended Tools\\nPopular tools used with LangChain include FAISS and Chroma for vector storage, and models such\\nas OpenAI, Gemini, or Hugging Face LLMs for response generation.\\n7. Conclusion\\nThis PDF serves as an ideal example dataset for building and testing a LangChain-based Q&A;\\nchatbot. Its clean structure and focused content ensure high-quality retrieval and accurate answers.')]\n"
     ]
    }
   ],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc8e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23112\\1944106899.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "#import langchain memory\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3145c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,    # Maximum size of each chunk\n",
    "                                             chunk_overlap=0)    # Number of characters to overlap between chunks\n",
    "docs=text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858af46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Free local embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d73a580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS  \n",
    "vectorstore = FAISS.from_documents(docs, embeddings)            # Create a FAISS vectorstore\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})    # Create a retriever and k means How many similar chunks to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06e3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant. \n",
    "Answer questions based on the documents provided by the retriever. \n",
    "If the information is not in the documents, answer from your own knowledge, \n",
    "but clearly indicate when you are doing so.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b01feca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "\n",
    "llm=GoogleGenerativeAI(temperature=0, model=\"models/gemini-2.5-flash-lite\", \n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a9b3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(  # Create a RetrievalQA chain and from_chain_type means use the specified chain type\n",
    "    llm=llm,\n",
    "    retriever=retriever,  \n",
    "    chain_type=\"stuff\" ,   # Use the \"stuff\" chain type to combine information from multiple sources\n",
    "    memory=memory,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "357ba897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework that helps developers build applications powered by language models. It supports document loading, text splitting, embeddings, vector databases, and retrieval-augmented generation (RAG).\n"
     ]
    }
   ],
   "source": [
    "query = \"What is langchain?\"\n",
    "response = qa_chain.invoke(query)\n",
    "print(response['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agentic_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
